---
title: "GEM500: Carbon and Biomass estimation"
author: "Tommaso Trotto (tommaso.trotto@ubc.ca)"
affiliation: "University of British Columbia, Department of Forest Resource Management"
date: "10/21/2024"
bibliography: references.bib
format:
  html:
    page-layout: full
    code-fold: true
    theme: flatly
    toc: true
    toc-float: true
    toc-location: left
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
```

# Forest biomass and carbon via remote sensing

Forest worldwide are integral part of the carbon cycle as sinks and sources of carbon during processes such as growth, decay, disturbance, and renewal. Their role is to capture carbon in the form of CO<sub>2</sub> from the atmosphere through photosynthesis and fix it into more stable organic structures such as roots, stems, leaves, vascular system, as so forth in the form of bounded carbon (carbon chemically bounded to other elements such as hydrogen or nitrogen). On average, the fixed carbon is estimated to make up 50% of the tree biomass, while the rest is mostly water, to oversimplify. Disturbances such as insect infestations, windthrow, and fires instead cause forests to release carbon back into the atmosphere as CO<sub>2</sub>, either by burning the standing wood or initiating decomposition processes. For example, the fire season in 2023 in Canada resulted in the release of roughly 480 Mt of carbon, which corresponds to approximately 23% of the global wildfire carbon emission for the year. A huge amount! This tells you how important it is to manage forests to limit or prevent the occurrence of these large-scale disturbances. So what can we do? In managed forests, forest interventions influence natural dynamics or carbon sequestering and release by planting, growing, or removing biomass.

As you may have hinted by now, it is crucial to keep track of carbon stocks at various scales, thus requiring different data sources, understand at what pace carbon moves, and what actors are involved in storing and releasing carbon, especially in face of exacerbating disturbance regimes resulting from climate change. The heat around carbon has led to the development of various carbon models calibrated for specific objectives (e.g. sequestration/release) and/or site conditions (e.g. boreal forest/tropical forest). In particular, for this lab our focus is on carbon models to estimate the amount sequestered by standing vegetation. These models may take a variety of data types depending on the scale of application. For example, we could work with inventory-based models, where the data is pulled from inventory information regarding individual stands or the entire landscape. This is what the Carbon Budget Model of the Canadian Forest Sector (CBM-CFS3) does. An aspatial carbon model that takes data from the forest inventory to estimate carbon stocks and stock changes. CBM-CDF3 is the tool that is currently used by the CFS to report on the forest carbon balance of Canada's managed forests (where we have inventory data). However, we are now taking a remote sensing program, so why are we talking about aspatial data? At Loon Lake we worked with data sources and types to answer different questions, so we should try to apply those concepts here. Thankfully, other spatial models have been developed over the years to account for the greater availability or remotely sensed data that go outside managed forest boundaries.

# Physiological Process Predicting Growth (3PG)

Physiological Process Predicting Growth (3PG) is a model developed by @landsberg1997 and subsequently extended by @coops1998 to allow remotely sensed data to be pulled in and therefore enlarge the scope and scale of the model. 3PG is a process-based model, which means it is deterministic in nature as it builds predictions based on a fixed series of well-known ecological processes that simulate the natural carbon dynamics in a forestry context, while having a clear ecological interpretations. The power of 3PG comes from ecological processes simplification, while maintaining their validity, and ability to scale with the data. 3PG is well suited for estimating biomass and carbon dynamics from newly grown forest patches. Meaning that 3PG really does its best at growing a forest from the ground up and gives you an estimate of stem, root, and foliage biomass at the end of a user-specified growth period. However, with this lab we are taking a step further by taking an historical perspective on 3PG and try to work out current biomass and carbon estimates assuming that the forest was planted many years ago. In a nutshell, we are going to try reconstructing highly simplified growth dynamics to get an estimate of today's carbon and biomass content of the forest in Malcolm Knapp.

In summary, the lab is structured into 2 sections:

1. Use 3PG to model tree growth until the end of the century considering a newly planted stand
2. Take an historical perspective on Malcolm Knapp's forests and try estimating the current biomass and carbon content

## Data sources and preparation

Before we dive in, we need to better understand at what scale 3PG operates and what data is therefore needed and where we could retrieve it from. The 3PG implementation we are going to use works is by @trotsiuk2020 (`r3PG`), works at monthly steps, and requires:
-   Climate data (temperature and precipitation)
-   Photoshyntetically-active solar radiation (PAR)
-   Number of frost days (FD)
-   Available soil water column (ASW)
-   Soil fertility
-   Elevation data (DEM - Digital Elevation Model)
-   Species growth parameters

Ok! That are quite a few things going on here. You may understand how not only do we need to gather this data from a multitude of sources, but also harmonize it into a format that works with `r3PG`. Thankfully, we already have almost all the data we need (@tbl-sources) in the form of raster layers at a 90m spatial resolution, we just need to prepare it. In addition to this data, `r3PG` requires specific input information we'll see down the road.

| Data               | Description                                                                         | Units              | Source             |
|--------------------|-------------------------------------------------------------------------------------|--------------------|--------------------|
| Climate*           | Minimum and maxium temperature and precipitation future predictions based on SSP2   | Celcius, mm        | @wang2016          |
| PAR*               | Amount of radiation that activates photosynthesis from 400 to 700 nm based on SSP2. | Mj/m<sup>2</sup>/d | @wang2016          |
| FD*                | Number of frost days                                                                | days               | @wang2016          |
| ASW (max)          | Max depth of available soil water                                                   | mm                 | @wang2016          |
| Soil fertility     | Soil fertility                                                                      |                    |                    |
| DEM                | Elevation data                                                                      | m                  | ASTER satellite    |
| Species Parameters | Species-specific growth parameters for species of interest                          |                    | Various literature |
: Summary and description of data available to run `r3PG`. *Available at monthly time steps. {#tbl-sources}

***Question 1: When working with multiple data source for time series analyses, what are some of the properties of your data (e.g. source, type) you would need to consider prior to conducting your analysis. Use a bullet point format to answer.***

The cool thing about `r3PG` is that it helps us keep track of carbon and biomass dynamics as our data varies over time, so in our case we could track it monthly! In addition to that, `r3PG` (as well as similar implementations) allows you to input source data predictions, like varying climate scenarios to observe how carbon and biomass change as climate changes. In this lab, we will first try to determine the amout of carbon stored at present and do a simple prediction on how it would change under a Shared Socioeconomic Pathway - 2 (SSP2) (@riahi2017) until the end of the century.

As mentioned earlier, one caveat you'll encounter with `r3PG` is the data format it requires to actually run the model in the background. Unfortunately, no raster data is directly supported, so we'll have to work around that and prepare our input data as required by this implementation, that is as `dataframes`. That means that every pixel in the raster layers will correspond to an entry in our dataframes. To maintain the spatial nature of the model, we'll have to sequentially run `r3PG` for each entry, such that each entry will have a corresponding carbon/biomass value and trend over time. Then, we transform these values back into a raster layer by assigning each estimate to a empty raster, conventionally starting from the top-left pixel to the bottom-right pixel.

## Part 1: Simulating forest growth

Let's get us started! We begin this first part of the lab by simulating tree growth from the ground up assuming that Malcolm Knapp has been completely harvested and replanted at the beginning of the year. We begin by preparing our `sites`, that is the area where the trees are planted. A `site` for `r3PG` is defined by a set of attributes, including latitude, elevation, soil class, and ASW (minimum, maximum, initial) within a defined growth period. This growth period represents the period the tree will grow for on the site. If we wanted to grow the trees from the ground up, a task 3PG excels at, we would set the begin of the growing period to today until the year we want the model to stop simulating, which for this lab is the end of the century. Remember that for the entire length of the period you must have data because `r3PG` can not deal with missing attributes. Alternatively, for the sake of this lab, we can assign a constant value and assume it won't change throughout the growing period. For example, we are giving a soil class of 2 (i.e. sandy soil), an initial ASW equal to the max ASW, and a minimum ASW of 0 millimeter.

Note how the raster layers were converted into `dataframes` and every single entry has now `site` information associated to it (@fig-dem, @tbl-site) to conserve the spatial nature of the model. Furthermore, some entries have NA values. We will see why we are keeping those later.

``` {r libraries}
#| warning: false
#| label: fig-dem
#| fig-cap: "Elevation map (m) for Malcolm Knapp"

library(r3PG)
library(terra)
library(dplyr)
library(tidyr)
library(tidyverse)
library(reshape2)
library(purrr)
library(lubridate)
library(ggplot2)
library(parallel)

# Note that, while there are many other ways to code these chunks, what we present is a very straightforward way, especially if you do not feel super comfortable with your coding skills.

# time frame
period <- '2011-2040'
from <- '2024-01'
to <- '2100-01'

# latitude
lat <- rast('data/Lat.tif')
coords <- crds(lat, na.rm = F)  # store coordinates for later
names(lat) <- 'latitude'
lat <- terra::as.data.frame(lat, na.rm = F)

# elevation
dem <- rast('data/DEM.tif')
plot(dem)
names(dem) <- 'altitude'
dem <- terra::as.data.frame(dem, na.rm = F)

# ASW
asw_max <- rast('data/ASW_Max.tif')
names(asw_max) <- 'asw_max'
asw_max <- terra::as.data.frame(asw_max, na.rm = F)
asw_min <- 0
asw_i <- asw_max
names(asw_i) <- 'asw_i'

# prepare sites
# the model wants all variable to be explicit
# if we don't have values, use a constant
site <- data.frame(latitude = lat,
                   altitude = dem,
                   soil_class = 2,
                   asw_i = asw_i,
                   asw_min = asw_min,
                   asw_max = asw_max,
                   from = from,
                   to = to)

```

``` {r site_df}
#| echo: false
#| label: tbl-site
#| tbl-cap: Tabular version of site data for Malcolm Knapp

head(site)
```

It's now time to prepare our species dataset! `r3PG` requires information on when the trees were planted, the planting density, the fertility of the soil, and what the initial seedling biomass was to start growing them. In Malcolm Knapp we are primarily working with Douglas fir, Western red cedar, and Western hemlock, so we'll input values for all these species. We set the planting date to the beginning of the year because we are just starting growing the trees. We then assume an initial number of stem per hectares of 5000 (conservative), and a biomass partitioning among stem, root, and foliage equal to 6, 3, and 1 Mg/ha. Note that because we have 3 species, we'll have 3 different biomass/carbon estimates, one for each species (@tbl-species).

``` {r species}
#| label: tbl-species
#| tbl-cap: Species dataset for every site in Malcolm Knapp

spp <- c('PSEU.MEN', 'THUJ.PLI', 'TSUG.HET')  # latin names

# soil fertility
ftl <- rast('data/FTL.tif')
names(ftl) <- 'fertility'
ftl <- terra::as.data.frame(ftl, na.rm = F)

# prepare species
species <- data.frame(spp1 = spp[1],
                      spp2 = spp[2],
                      spp3 = spp[3],
                      planted = from,
                      fertility = ftl,
                      stems_n = 1700,
                      biom_stem = 6,
                      biom_root = 3,
                      biom_foliage = 1)
# reshape dataset so all data is repeated for each species
# important for downstream tasks
species <- melt(species,
                id.vars=names(species)[4:ncol(species)],
                value.name='species') %>%
  select(-variable) %>%
  relocate(species)
head(species)
```

For now that's all we need for the tree species. Let's start working on the climate modifiers and input what the projected climate will be until the end of the century. This step will allow `r3PG` to develop carbon and biomass projections in line with well established climate scenarios. As mentioned earlier, we have data using a SSP2 scenario. Note that for simplicity, we are considering the the climate data between 2011 and 2040 remain constant until the end of the century. In a real-case situation, you would want to follow the 20-year climate normals and re-run the model for every 20-year period while "carrying over" the biomass from the previous period in order to continuously grow the trees (instead of starting from the ground up at every 20-year interval) (@tbl-climate).

```{r climate}
#| warning: false
#| label: tbl-climate
#| tbl-cap: Climate dataset for every site in Malcolm Knapp

# I'll implement a function to make the code less lengthy, but any other approach would work
needed <- c('Tmin', 'Tmax', 'PPT', 'Rad', 'NFFD')
build <- function(need) {
  f <- list.files('./data/ssp2',
                pattern = paste0(need, '.*', period),
                full.names = T)
  # open each file
  dfs <- lapply(seq_along(f), function(i) {
    r <- rast(f[i])
    names(r) <- i
    r <- terra::as.data.frame(r, na.rm = F)
  })
  # merge into single dataframe and convert to long format
  out <- dfs %>% 
    bind_cols() %>% 
    melt() %>%
    rename(!!sym(need) := value) %>%
    select(-variable)
}
climate <- lapply(needed, build)

# put them in a single dataframe that can be used later on
climate <- bind_cols(climate)

# rename to match r3PG expectations
names(climate) <- c('tmp_min', 'tmp_max', 'prcp', 'srad', 'frost_days')

head(climate)
```

Climate data is now ready! We are dealing with quite some data here. More than 100K records! Some will be NA values, but we'll deal with those later to maintain the order with which we collected the data in the first place. This is important to maintain the `r3PG` spatial nature. That is, each pixel has a record of data and each pixel will have a corresponding carbon/biomass value. Pretty cool!

We are down to the last step before running the model! Now it's time to input the species growth parameters (@tbl-parameters) (note that only 6 are displayed, but there are dozens!). We don't need to know in details what all the parameters mean because there are lots. If you're interested, explore the data that comes with this lab or check `i_parameters` for a description of each. A parameter that is very important to know is quantum canopy efficiency (aka "alpha"). Simply put, this parameters regulates how efficient the foliage in the canopy is to utilize incoming solar radiation to photosynthesize nutrients. Changing this parameter by any small amount will have a large impact on the biomass/carbon predictions. In general, these parameters allow us to develop species-specific growth curves with an associated carbon/biomass content. This step of our implementation is pretty easy, the file is ready to go, so we simply import it and pick the species we are interested in.

``` {r parameters}
#| label: tbl-parameters
#| tbl-cap: Parameter dataset for each of the 3 species of interest in Malcolm Knapp

# species file
select_all <- c('parameter', spp)
param <- read_csv('data/Final_Parameters.csv', show_col_types = F) %>%
  select(all_of(select_all))
head(param)
```

## Model developement

It's finally time to dive in into the model itself! We have all the data we need, each record associated to a single pixel. One caveat we are going to face now is related to the spatial nature we seek to preserve from `r3PG`. In principle, 3PG was not developed to be a spatial model, but it can take remotely sensed data to extend its scope and become spatial (@coops1998). The workaround to that is to treat each pixel *individually* and get a value for each! To do so, we'll have to do some iterations and run the same model over and over for each pixel. This can be quite tedious if you're working with a large dataset, but our study area is pretty small, so it shouldn't be too bad. 

If you recall from earlier, we kept all NA values, why? Each raster layers we imported is likely to have a number of pixels with NA value for a variety of reasons. However, it is quite unlikely that every raster we imported has the exact same empty pixels. Because we must have non-empty records for every pixel, we first compile them all together, and during the model iterations we check for the presence of NA. If none, we continue to run it. This way, we preserve the original location of the pixel and avoid running the model if there are missing values. Note that this workaround is straightforward when you work with small amounts of data. When dealing with much larger datasets, you may need to figure out other ways to work this out, simply because of the amount of data you'd work with.

To work more efficiently, we will write a function to process each pixel sequentially and then we'll do some simple code parallelization to take advantage of your machine's core and speed everything up a little. Running the model now is pretty straightforward. Just use the `run_3PG` function, so let's set this up. First, we'll do a single-pixel run to show you how this works if Douglas Fir were planted. Next, we will write the actual model call. We have to fix the climate data such that for 1 pixel we have 12 months of data, which will be repeated for all years until the end of the century. We are going to accomplish that with the `prepare_climate` function.

```{r pixel}
# year climate for 1 non-empty pixel
offset <- 8960
pixels <- seq(41, offset*12, offset)
climate_pixel <- climate[pixels, ]
climate_pixel <- prepare_climate(climate_pixel, from = from, to = to)

# example on 1 non-empty pixel
output <- run_3PG(site = site[41,],
                  species = species[41, ],
                  climate = climate_pixel,
                  parameters = param[, 1:2],
                  check_input = T,
                  df_out = T)

# extract stem biomass and carbon
stem_bc <- output %>%
  filter(variable == 'biom_stem')
stem_bc$carbon <- stem_bc$value / 2
```

***Question 2: Model development generally comes with a series of underlining assumptions. In our case, we observed a logarithmic growth that plateaus around the year 2060. Briefly describe 4 major data assumptions that we made in developing this model and how they could be more explicitly addressed.***

Great! We finally managed to run 3PG until the end of the century! As you can observe from the output, we have biomass and carbon values in tDM/ha ("tonnes of dry matter per hectare") at the end of each month. Let's do some plotting to better see what the trend is over time if Malcolm Knapp were planted with only Douglas Fir (@fig-biomass).

```{r plot}
#| label: fig-biomass
#| fig-cap: "Biomass growth for Douglas Fir from 2024-01 to 2100-01 for 1 pixel in Malcol Knapp"
#| echo: false

p <- ggplot(stem_bc, aes(date, value)) +
  geom_line(linewidth = 1) +
  theme_classic() +
  labs(x = "Years", y = "Biomass (tDM/ha)")
p
```

It seems we are seeing a nice increase in biomass over time, with a plateau around year 2060. Why is that?

Now to a more fun task. We have to repeat this operation for every single pixel in the area. Let's write a function to do all this and run it for every pixel. We are going to produce quite a few new data here. In the code chunk below we are simply going to repeat what we did for the single-pixel run, and then run multiple instances of it for each pixel, assuming that the only species present is Douglas Fir. We will then make a map of the predicted biomass in January 2100. To do so, we will have to create a new raster object starting from the biomass dataframe we have (@fig-biomass-map). You can achieve this with the package `terra` that we previously used.

``` {r model}
#| label: fig-biomass-map
#| fig-cap: "Map of biomass prediction (tDM/ha) for Douglas Fir in Malcolm Knapp in 2100."

# prepare function to run the model
# we repeat the same code of above for simplicity
# note that 
run_model <- function(pixel, from, to) {
  # if empty, don't run
  if (isFALSE(complete.cases(site[pixel, ]))) {
    df_null <- data.frame(date = as.Date('1900-01-01'),
                          species = spp[1],
                          group = 'stocks',
                          variable = 'biom_stem',
                          value = NA)
    return(df_null)
  }
  # prepare climate data for site
  pixels <- seq(pixel, offset*12, offset)
  climate_pixel <- climate[pixels, ]
  climate_pixel <- prepare_climate(climate_pixel, from = from, to = to)
  output <- run_3PG(site = site[pixel,],
                    species = species[pixel, ],
                    climate = climate_pixel,
                    parameters = param[, 1:2],
                    check_input = T,
                    df_out = T)
  stem_bc <- output %>%
    filter(variable == 'biom_stem')
  return(tail(stem_bc, 1))  # year 2100
}

# run the model for every pixel
# let's take advantage of more cores in your machine to speed everything up
output <- mcmapply(run_model,
                   pixel = seq_len(nrow(site)),
                   from = rep(from, nrow(site)),
                   to = rep(to, nrow(site)),
                   mc.cores = 10L,
                   mc.cleanup = T,
                   SIMPLIFY = F) %>%
  bind_rows()

# make a raster object
r <- output %>%
  select(value) %>%
  rename(z = value) %>%
  mutate(x = coords[, 1],
         y = coords[, 2],
         .before = z) %>%
  rast(type='xyz', crs='epsg:3005')

plot(r)
```

To repeat this analysis for the other species, we run the same code but change the input species. If you want to be more fancy, you can re-write the function to accept multiple species and output a raster stack, that is a collection of raster data with the same coordinate reference system, extend, and resolution stored in a single object.


## Part 2: Historical biomass reconstruction

Now that we have completed the first part of the lab, recall we want to know what is today's biomass across Malcolm Knapp. To do that using 3PG, we have to reconstruct the history of the site by retrieving the age of the trees and assume the model's variables remained unchanged for the entire length of the simulation. Thankfully, we have age information for Canada's treed area at a 30m resolution (@maltman2023) (@fig-age), which is great! Note the presence of gaps due to (mostly) absence of vegetation. Given this piece of information, let's refine the length of the growth period for each pixel from when the trees started growing to the beginning of 2024. We do this because each pixel will have a different age value compared to its neighbors. Then, we'll adjust the site and species values to account for the age value of each pixel.

``` {r age}
#| label: fig-age
#| fig-cap: "Age distribution at 30m for Malcolm Knapp"

# import age
age <- rast('data/age.tif')
plot(age)
names(age) <- 'age'
age <- terra::as.data.frame(age, na.rm = F)

# adjust from and to
to <- as.Date("2024-01-01")
dates <- years(as.integer(age$age) + 1)  # round it up
from <- to - dates
from <- format(from, "%Y-%m")

# fix site dataset
site$from <- from
site$to <- to

# fix species dataset
species$planted <- from
```

Now that we have fixed the site and species dataset with the new age values, assuming that they were all planted, we have to change the projection length for the climate dataset inside our `run_model` function and then we are ready to run it and estimate today's biomass per pixel!

``` {r biomass_today}
#| label: fig-biomass-today
#| fig-cap: "Map of biomass prediction (tDM/ha) for Douglas Fir in Malcolm Knapp in 2024."

output <- mcmapply(run_model,
                   pixel = seq_len(nrow(site)),
                   from = from,
                   to = rep(to, nrow(site)),
                   mc.cores = 10L,
                   mc.cleanup = T,
                   SIMPLIFY = F) %>%
  bind_rows()

# make a raster object
r <- output %>%
  select(value) %>%
  rename(z = value) %>%
  mutate(x = coords[, 1],
         y = coords[, 2],
         .before = z) %>%
  rast(type='xyz', crs='epsg:3005')

plot(r)
```

***Question 3: One would expect age to be a pretty good predictor of biomass in coastal regions. However, the output of the 3PG projection seems to suggest otherwise. This stresses the importance of model validation following development to validate your results. What additional information do you think would be needed to make sure we are getting an accurate representation of biomass and carbon in Malcolm Knapp?***

Note how the results we obtained closely mirror the predicted biomass in 2100 we estimated earlier. This results show how dependent the model is on the input data. By giving it the same climate dataset, which is drawn by climate projection based on SSP2 (@tbl-sources), the output we obtain is almost identical, except for missing values where we don't have age information and the darker areas in the north-west portion of Malcolm Knapp that have probably been harvested recently.


## Part 3: Comparison with our plots

Now that we have calculated both projected and current biomass content at the pixel level for Malcolm Knapp, let's experiment a bit by comparing the results we obtained from Lab 1 at Loon Lake with what we just produced. Our aim here is to compare the biomass values we just obtained with what we can infer from the plots we set up a while ago. In particular, we are going to extract volume information for each tree in your plots based on diameter at breast height (DBH) and height - by assuming stems to resemble a perfect cone, which is unrealistic (@eq-volume) - and compare them to the pixel-level values we have. This would help us reflect on the data type, source, and scale at which such information is representative of the local environment, that is whether we trust the data we have for the scale we are working at.


$$ \frac{1}{3}\pi r^{2} h $$ {#eq-volume}

So let's start by retrieving the data we have on the trees we sampled at Loon Lake.

```{r plots}
#| message: false

library(sf)
library(googlesheets4)
library(readxl)
library(leaflet)

# no authentication required
gs4_deauth()

# read sheet
tally <- read_sheet('1M2BZIHRX_D_T4J8ji0B8pVnSJr6yMEqdvVCB9ryAWfc', sheet='DATA')

# add volume info
tally <- tally %>%
  select(-notes) %>%
  mutate(across(c(height, dbh), as.numeric)) %>%
  drop_na() %>%
  mutate(volume = (1/3) * pi * ((dbh / 2 / 100) ^ 2) * height)

# get average per plot
by_plot <- tally %>%
  group_by(plot) %>%
  summarize(observed = mean(volume))
by_plot
```

To compare the values we got in the field with the pixel-level biomass content, we need to georeference the plot centers, that is we have to assign geospatial information to each plot in order to properly place them on the map. We did this in the field when we collected the plot center with a GPS receiver, so let's add this information to our current dataframe (@fig-plots).

```{r georef}
#| label: fig-plots
#| fig-cap: "Georeferenced plots in Malcolm Knapp"

geo <- read_excel('data/plot_centers.xlsx') %>%
  filter(plot %in% c(1, 2, 5))
by_plot <- by_plot %>%
  mutate(x = geo$x,
         y = geo$y) %>%
  st_as_sf(coords = c("x", "y"), crs = 4326) %>%
  st_transform(crs = 3005) %>%  # same CRS of raster
  st_buffer(11.28)  # plot radius

# plot
m <- plet(vect(by_plot))
m
```

To finalize the comparison, let's take the pixels corresponding to the location of our plots. We can do this by using the geospatial information attached to every plot and calculating zonal statistics, that is statistics associated to an area in the map marked by another raster or polygon. We do this so that the values we extract from the raster are aggregated for the area covered by our individual polygons. Note that this is a good practice when you have overlays larger than a pixel. Our plots are very tiny compared to the 90m pixels of the biomass raster, so in this case we are using `terra::extract` with `touches = TRUE` to make sure that if the plots happens to touch more than 1 pixel, all are returns (check `terra::zonal` for comparison).

``` {r comparison}
#| label: fig-comparison
#| fig-cap: "Comparison of biomass values retrieved from plot data versus pixel-level estimations with r3PG"

# extract pixel-level information for each plot
predicted <- terra::extract(r,
                            vect(by_plot),
                            touches = T) %>%
  group_by(ID) %>%
  summarise(predicted = mean(z)) %>%
  rename(plot = ID) %>%
  mutate(plot = by_plot$plot)

# comparison
comparison <- left_join(predicted, st_drop_geometry(by_plot), by = 'plot')
comparison
```

***Question 4: Reflect on the comparison between field measurements and remotely sensed data. Answer with a bullet point each the following: (1) What should one consider when comparing field observation to satellite data? (2) What data would you use for what purpose and why? Provide 2 examples for the latter.***

### References

::: {#refs}
:::























